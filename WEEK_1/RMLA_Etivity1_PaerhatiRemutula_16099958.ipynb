{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "RepMLA_Lab-1_13.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.10 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "509eb6a060868bfafdc22a487b44ad6b0ff2d2f8f2e450dcc280f29fc79f69d2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1vK33e_EqaHgBHcbRV_m38hx6IkG0blK_\" width=\"350\"/>\n",
        "</div> \n",
        "\n",
        "#**Artificial Intelligence - MSc**\n",
        "CS6501 - MACHINE LEARNING AND APPLICATIONS\n",
        "#**Business Analytics - MSc**\n",
        "ET5003 - MACHINE LEARNING APPLICATIONS \n",
        "##***Annual Repeat***\n",
        "###Instructor: Enrique Naredo\n",
        "\n",
        "###RepMLA_Lab-1.13\n",
        "\n",
        "Student ID: 16099958\n",
        "\n",
        "Student name: Paerhati Remutula"
      ],
      "metadata": {
        "id": "Qvexyww7a-0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# E-tivity: K-Nearest Neighbors"
      ],
      "metadata": {
        "id": "pLNHPNlXlUk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "The goal is to implement the K-nearest neighbors algorithm (a supervised machine learning algorithm) and apply it to a real dataset. Along the way you should familiarize yourself with some of the terminology you have read in the note. You will also get a chance to practice with Python and working with large datasets.\n"
      ],
      "metadata": {
        "id": "wbGmVVOWlUk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset introduction\n",
        "\n",
        "The handwritten digit recognition is the ability of computers to recognize human handwritten digits. It is a hard task for the machine because handwritten digits are not perfect and can be made with many different flavors. The handwritten digit recognition is the solution to this problem which uses the image of a digit and recognizes the digit present in the image.\n",
        "\n",
        "The dataset for this E-tivity is a set of handwritten digits from zip codes written on hand-addressed letters (MNIST-Modified National Institute of Standards and Technology database). \n",
        "\n",
        "Read about this dataset by going to the Elements of Statistical Learning website, <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">ESL</a>, then clicking on the `Data` tab, then clicking on the `Info` for the zip code dataset (the last dataset). \n",
        "\n",
        "Use the command less in the terminal to view the beginning of each file. Both datasets have the same format: the first column is the \"label\" (or class) (here an integer between 0 and 9, inclusive, that corresponds to the identity of a hand-written zip code digit), and the rest of each row is made up of gray-scale values corresponding to the image of this hand-written digit.\n",
        "\n",
        "One useful technique is to load a dataset from a file into a numpy array. Here is an example:"
      ],
      "metadata": {
        "id": "0TpgU67FlUk3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import numpy as np\n",
        "train_data = np.loadtxt(\"datasets/mnist.train\") #\"path/to/train/file\"\n",
        "test_data  = np.loadtxt(\"datasets/mnist.test\")\n",
        "\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7291, 257)\n",
            "(2007, 257)\n"
          ]
        }
      ],
      "metadata": {
        "id": "fWOGDvcolUk4",
        "outputId": "f961bf17-07e8-49dc-a9fc-32c9cc220d47"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "train_data #each row is a different image"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6.   , -1.   , -1.   , ..., -1.   , -1.   , -1.   ],\n",
              "       [ 5.   , -1.   , -1.   , ..., -0.671, -0.828, -1.   ],\n",
              "       [ 4.   , -1.   , -1.   , ..., -1.   , -1.   , -1.   ],\n",
              "       ...,\n",
              "       [ 3.   , -1.   , -1.   , ..., -1.   , -1.   , -1.   ],\n",
              "       [ 0.   , -1.   , -1.   , ..., -1.   , -1.   , -1.   ],\n",
              "       [ 1.   , -1.   , -1.   , ..., -1.   , -1.   , -1.   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "metadata": {
        "id": "_--pMIHJlUk5",
        "outputId": "f738db1f-27d8-460a-fe9a-e82dd96c441a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first column is the class, it tells us which digit we will find in the image, as you can see hereafter for the first row (it is a 6):"
      ],
      "metadata": {
        "id": "336Q8CIUlUk6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "row = 0\n",
        "flatten_image = train_data[row, 1:]  # each 16x16 image has been flatten into a vector\n",
        "im = flatten_image.reshape(16, 16)\n",
        "print(im)  # as a matrix\n",
        "plt.gray()\n",
        "plt.imshow(im)  # as an image"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.    -1.    -1.    -1.    -1.    -1.    -1.    -0.631  0.862 -0.167\n",
            "  -1.    -1.    -1.    -1.    -1.    -1.   ]\n",
            " [-1.    -1.    -1.    -1.    -1.    -1.    -0.992  0.297  1.     0.307\n",
            "  -1.    -1.    -1.    -1.    -1.    -1.   ]\n",
            " [-1.    -1.    -1.    -1.    -1.    -1.    -0.41   1.     0.986 -0.565\n",
            "  -1.    -1.    -1.    -1.    -1.    -1.   ]\n",
            " [-1.    -1.    -1.    -1.    -1.    -0.683  0.825  1.     0.562 -1.\n",
            "  -1.    -1.    -1.    -1.    -1.    -1.   ]\n",
            " [-1.    -1.    -1.    -1.    -0.938  0.54   1.     0.778 -0.715 -1.\n",
            "  -1.    -1.    -1.    -1.    -1.    -1.   ]\n",
            " [-1.    -1.    -1.    -1.     0.1    1.     0.922 -0.439 -1.    -1.\n",
            "  -1.    -1.    -1.    -1.    -1.    -1.   ]\n",
            " [-1.    -1.    -1.    -0.257  0.95   1.    -0.162 -1.    -1.    -1.\n",
            "  -0.987 -0.714 -0.832 -1.    -1.    -1.   ]\n",
            " [-1.    -1.    -0.797  0.909  1.     0.3   -0.961 -1.    -1.    -0.55\n",
            "   0.485  0.996  0.867  0.092 -1.    -1.   ]\n",
            " [-1.    -1.     0.278  1.     0.877 -0.824 -1.    -0.905  0.145  0.977\n",
            "   1.     1.     1.     0.99  -0.745 -1.   ]\n",
            " [-1.    -0.95   0.847  1.     0.327 -1.    -1.     0.355  1.     0.655\n",
            "  -0.109 -0.185  1.     0.988 -0.723 -1.   ]\n",
            " [-1.    -0.63   1.     1.     0.068 -0.925  0.113  0.96   0.308 -0.884\n",
            "  -1.    -0.075  1.     0.641 -0.995 -1.   ]\n",
            " [-1.    -0.677  1.     1.     0.753  0.341  1.     0.707 -0.942 -1.\n",
            "  -1.     0.545  1.     0.027 -1.    -1.   ]\n",
            " [-1.    -0.903  0.792  1.     1.     1.     1.     0.536  0.184  0.812\n",
            "   0.837  0.978  0.864 -0.63  -1.    -1.   ]\n",
            " [-1.    -1.    -0.452  0.828  1.     1.     1.     1.     1.     1.\n",
            "   1.     1.     0.135 -1.    -1.    -1.   ]\n",
            " [-1.    -1.    -1.    -0.483  0.813  1.     1.     1.     1.     1.\n",
            "   1.     0.219 -0.943 -1.    -1.    -1.   ]\n",
            " [-1.    -1.    -1.    -1.    -0.974 -0.429  0.304  0.823  1.     0.482\n",
            "  -0.474 -0.991 -1.    -1.    -1.    -1.   ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8e4873fcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "id": "UyPvWvnzlUk6",
        "outputId": "844b1f57-b4f1-4d07-bdce-8e0deb5eed0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test set is similar:"
      ],
      "metadata": {
        "id": "AktYMkHQlUk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "test_data"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 9., -1., -1., ..., -1., -1., -1.],\n",
              "       [ 6., -1., -1., ..., -1., -1., -1.],\n",
              "       [ 3., -1., -1., ..., -1., -1., -1.],\n",
              "       ...,\n",
              "       [ 4., -1., -1., ..., -1., -1., -1.],\n",
              "       [ 0., -1., -1., ..., -1., -1., -1.],\n",
              "       [ 1., -1., -1., ..., -1., -1., -1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "metadata": {
        "id": "rIn2eSfSlUk7",
        "outputId": "2989e2e3-54ab-492a-e071-25cbd7c0c4bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Filter the Data\n",
        "To start, we will just consider two classes, but here we have 10. We will get to such problems later, but for now, devise a way to retain only the rows which have label 2 or 3. Do this for both the train and test data.\n",
        "\n",
        "One important note: it may be convenient to relabel the 2 to -1 and the 3 to +1, since this will work better with our methods later on (but you do not have to do this).\n"
      ],
      "metadata": {
        "id": "CQ3I2AgxlUk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "train_23 = []\n",
        "for row in train_data:\n",
        "    if row[0] in [2, 3]:\n",
        "        train_23.append(row)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "train_23 = np.asarray(train_23)\n",
        "train_23.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1389, 257)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "test_23 = []\n",
        "for row in test_data:\n",
        "    if row[0] in [2, 3]:\n",
        "        test_23.append(row)\n",
        "\n",
        "test_23 = np.asarray(test_23)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(364, 257)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "test_23[0]  # take a peak"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3.   , -1.   , -1.   , -1.   , -0.593,  0.7  ,  1.   ,  1.   ,\n",
              "        1.   ,  1.   ,  0.853,  0.075, -0.925, -1.   , -1.   , -1.   ,\n",
              "       -1.   , -1.   , -1.   , -0.553,  0.998,  1.   ,  1.   ,  1.   ,\n",
              "        1.   ,  1.   ,  1.   ,  1.   ,  0.961, -0.076, -0.999, -1.   ,\n",
              "       -1.   , -1.   , -1.   ,  0.228,  1.   ,  0.849, -0.15 , -0.705,\n",
              "       -1.   , -0.85 , -0.333, -0.072,  0.929,  1.   , -0.451, -1.   ,\n",
              "       -1.   , -1.   , -1.   , -0.586,  0.777, -0.524, -1.   , -1.   ,\n",
              "       -1.   , -1.   , -1.   , -1.   ,  0.344,  1.   ,  0.544, -0.999,\n",
              "       -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   ,\n",
              "       -1.   , -1.   , -1.   , -0.803,  0.93 ,  1.   ,  0.65 , -0.999,\n",
              "       -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   ,\n",
              "       -1.   , -1.   , -0.579,  0.821,  1.   ,  1.   , -0.131, -1.   ,\n",
              "       -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   ,\n",
              "       -0.621,  0.156,  0.934,  1.   ,  1.   ,  0.575, -0.933, -1.   ,\n",
              "       -1.   , -1.   , -1.   , -1.   , -1.   , -0.952, -0.176,  0.602,\n",
              "        1.   ,  1.   ,  1.   ,  0.952, -0.093, -1.   , -1.   , -1.   ,\n",
              "       -1.   , -1.   , -1.   , -1.   , -1.   , -0.41 ,  1.   ,  1.   ,\n",
              "        1.   ,  1.   ,  1.   ,  0.792, -0.715, -1.   , -1.   , -1.   ,\n",
              "       -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -0.345,  0.333,\n",
              "       -0.05 , -0.333, -0.172,  0.606,  0.95 , -0.101, -1.   , -1.   ,\n",
              "       -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   ,\n",
              "       -1.   , -1.   , -1.   , -0.838,  0.548,  1.   , -0.266, -1.   ,\n",
              "       -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   , -1.   ,\n",
              "       -1.   , -1.   , -1.   , -1.   , -0.285,  1.   ,  0.729, -1.   ,\n",
              "       -1.   , -1.   , -1.   , -0.611, -0.944, -1.   , -1.   , -1.   ,\n",
              "       -1.   , -1.   , -1.   , -0.556,  0.943,  1.   ,  0.779, -0.943,\n",
              "       -1.   , -1.   , -0.943,  0.779,  0.555, -0.333, -0.333, -0.333,\n",
              "       -0.166,  0.389,  1.   ,  1.   ,  1.   ,  1.   ,  0.497, -1.   ,\n",
              "       -1.   , -1.   , -1.   ,  0.507,  1.   ,  1.   ,  1.   ,  1.   ,\n",
              "        1.   ,  1.   ,  1.   ,  1.   ,  0.83 ,  0.053, -0.946, -1.   ,\n",
              "       -1.   , -1.   , -1.   , -0.941,  0.059,  0.615,  1.   ,  1.   ,\n",
              "        0.717,  0.333,  0.162, -0.393, -1.   , -1.   , -1.   , -1.   ,\n",
              "       -1.   ])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement K-nearest neighbors\n",
        "\n",
        "The main goal of the E-tivity is to implement the K-nearest neighbors classifier to predict the class of each example from the test dataset. Exactly how you implement this part is up to you, but your code should be decomposed into functions, well commented, and easy to understand. Here are some suggestions:\n",
        "\n",
        "**Classification**\n",
        "Create a function that takes as input the train set, (at least) a test example and an integer K, and outputs a prediction based on a nearest-neighbor classifier. This function will loop through all the training examples, find the distance between each one and the input test example, and then find the K nearest neighbors (you are welcome to use numpy sorting methods, but look up how they work first). For this subroutine, we will need a distance function. In practice, you have to implement the algorithm 3 in <a href=\"http://ciml.info/dl/v0_99/ciml-v0_99-ch03.pdf\">Duame</a> (pag. 33).\n",
        "\n",
        "\n",
        "You should implement a Python function like:"
      ],
      "metadata": {
        "id": "ZPNOPCNrlUk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student note: The implementation below is too slow, there is an optimised version of functions get_euc_distance() and KNN() at the later part of this notebook. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "def get_euc_distance(A, B):\n",
        "    \"\"\"\n",
        "    For given array A and B, return the euclidean distance. \n",
        "    \"\"\"\n",
        "\n",
        "    # The line below should be read from right to left: \n",
        "    #  for each pair (a, b) in the combination of A, B, \n",
        "    #  calculate (a - b) and then raise to the power of 2, \n",
        "    #  Because this 'for' loop is wrapped in a square bracket, \n",
        "    #  the outcome of each iteration in the loop will become \n",
        "    #  a list. \n",
        "    #  Then we use np.sum to get the sum of this list. \n",
        "    #  Finally return the square root of this sum. \n",
        "    return np.sqrt(np.sum([(a - b) ** 2 for a, b in zip(A, B)]))\n",
        "\n",
        "\n",
        "def KNN(train: list, test: list, K: int) -> list:  \n",
        "    \"\"\"\n",
        "    Given a training set, one or more test cases, and an integer K, \n",
        "    predict the class of each test case using K-nearest-neighbours \n",
        "    algorithm. \n",
        "\n",
        "    @param train: a list of training samples, can contain multiple classes, \n",
        "                    each element in this list is a sub-list with its first \n",
        "                    item being the class, and following 256 elements combined \n",
        "                    is the training sample. \n",
        "    @param test: a list of (one or more) test cases. \n",
        "    @param K: integer number indicating how many neighbours to look at. \n",
        "\n",
        "    @return: a list of prediction result, this list has the same size as the \n",
        "                input parameter 'test' \n",
        "    \"\"\"\n",
        "    if len(test) == 0:  # if the test parameter is empty, return an empty list. \n",
        "        print('No test case given. ')\n",
        "        return []\n",
        "    \n",
        "    prediction_results = []  # place-holder list for the classification result\n",
        "    \n",
        "    test_iter_count = 0\n",
        "    for test_case in test:  # there could be multiple test cases\n",
        "        # extract the test class\n",
        "        test_class = test_case[0]\n",
        "        # extract the test sample\n",
        "        test_case = test_case[1:]  \n",
        "\n",
        "        # calculate distances between this test_case and each of training examples. \n",
        "        distances = []\n",
        "        for train_sample in train:\n",
        "            train_class = train_sample[0]  # extract the class for later use\n",
        "            train_sample = train_sample[1:]  # now exclude the class from the train_sample var. \n",
        "\n",
        "            # now append (as a small list) the distance between this test case and train sample, \n",
        "            # and this train sample's class. \n",
        "            distances.append([get_euc_distance(test_case, train_sample), train_class])\n",
        "        \n",
        "        # sort the distances array so smallest distance comes first. \n",
        "        # using lambda function meaning for each of the element 'x' in the list, only look at \n",
        "        # the first item in 'x' (shown by x[0], being the distance) when sorting. \n",
        "        distances = sorted(distances, key=lambda x : x[0]) \n",
        "\n",
        "        # Now take the first K values in the 'distances' list and vote on its classes\n",
        "        # This is where converting the training samples' labels to be -1 and +1 comes in \n",
        "        # handy as Enrique mentioned above, as after the voting, we can just look at \n",
        "        # the sign of the sum. \n",
        "        # As I didn't make this change to the dataset, I need to keep a vote count manually. \n",
        "        vote_count = {}  # store the votes as key-value pairs\n",
        "\n",
        "        for res in distances[:K]: \n",
        "            if res[1] in vote_count:\n",
        "                # if this class exists in my count set, update the count value by 1\n",
        "                vote_count[res[1]] += 1\n",
        "            else:\n",
        "                # if not, add this class to the set and give it a value of 1\n",
        "                vote_count[res[1]] = 1\n",
        "\n",
        "        # try to find the max count and its type in the vote_count set\n",
        "        max_count, max_type = 0, None  # start with 0\n",
        "        for key, val in vote_count.items():\n",
        "            if val > max_count:\n",
        "                max_count = val\n",
        "                max_type = key\n",
        "\n",
        "        prediction_results.append(max_type)\n",
        "\n",
        "        test_iter_count += 1\n",
        "        print(f'KNN: K - {K}, total tests: {len(test)}, current: {test_iter_count / len(test) * 100}%')\n",
        "\n",
        "    return prediction_results"
      ],
      "outputs": [],
      "metadata": {
        "id": "0UOu4rRklUk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distance function**\n",
        "An important part of many machine learning methods is the concept of \"distance\" between examples. We often phrase this as a \"metric\" on our inputs. Create a function that takes as input two examples (any two examples, although in this case we will use it with one test and one train), and outputs the distance (we will use Euclidean for now) between them. Although there are many built-in functions the perform this task, please implement your distance function from scratch. However, you are welcome to use numpy functions as part of it (for example, you may use np.sum and similar functions, but look up how they work first).\n"
      ],
      "metadata": {
        "id": "BIKGEbjRlUk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student note\n",
        "The distance function is implemented in the code cell above, before the KNN function. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the KNN algorithm"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "# take a look at the classes of the first 10 tests\n",
        "print([row[0] for row in test_23[:10]])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "KNN(train=train_23, test=test_23[:10], K=3)  # test with K=3, and only the first 10 test cases "
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Quantify the accuracy**\n",
        "Loop through all the filtered test examples, using your classification function to predict the label for each one. Also create a way of determining if the prediction was correct or not, based on the labels of the test data. Compute the fraction or percentage of correctly predicted examples. How does this change as $K$ varies? Try $K$ 1-10 (at least) and record the accuracy.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantify the accuracy"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "# Try with K from 1 to 50\n",
        "results = []  # place holder for K values, and its accuracy\n",
        "\n",
        "# extract all test classes\n",
        "test_classes = [row[0] for row in test_data]\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "source": [
        "max_K = 25\n",
        "for k_iter in range(1, max_K + 1):\n",
        "    # use all train available and all tests available\n",
        "    test_res = KNN_optimised(train=train_data, test=test_data, K=k_iter)\n",
        "\n",
        "    # compare the test results with the known test classes and calculate accuracy\n",
        "    #  the line below means for each pair (a, b) in the combination of \n",
        "    #  test_classes and test results, check if a == b, if yes, add a '1' to a list\n",
        "    #  finally take the sum of this list of ones. \n",
        "    correct_count = sum([1 for a, b in zip(test_classes, test_res) if a == b])\n",
        "\n",
        "    # append this K, and the accuracy (correct count over all test cases) to the results\n",
        "    results.append([k_iter, correct_count / len(test_classes)])\n",
        "\n",
        "    # show where we are during the process, as well as the accuracy from this iteration\n",
        "    print(f'K: {k_iter} ({k_iter / max_K * 100:.2f}%) -- {results[k_iter - 1][1]}')"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student note: The above process is too slow, takes about 40 minutes to complete one iteration, I didn't wait for it. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The optimised version of get_euc_distance() and KNN()"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "source": [
        "from scipy.spatial import distance\n",
        "def get_euc_distance_optimised(A, B):\n",
        "    \"\"\"\n",
        "    For given array A and B, return the euclidean distance. \n",
        "    \"\"\"\n",
        "\n",
        "    # check if 2 arrays are of the same size\n",
        "    if len(A) != len(B):\n",
        "        raise ValueError('Two arrays need to be the same size. ')\n",
        "    # convert both input to numpy array\n",
        "    A = np.asarray(A)\n",
        "    B = np.asarray(B)\n",
        "\n",
        "    # get the difference between each pair\n",
        "    diff = np.subtract(A, B)\n",
        "    # raise each difference by the power of 2\n",
        "    squared = np.power(diff, 2)\n",
        "    # finally return the sqrt of sum\n",
        "    return np.sqrt(np.sum(squared))\n",
        "\n",
        "def KNN_all_train_test(train: list, test: list) -> list:\n",
        "    \"\"\"\n",
        "    This is a function that returns all distances for each test cases, then \n",
        "    in the function below \"KNN_optimised\", we don't need to calculate these \n",
        "    distances for every K, we just need to calculate once, and then take the \n",
        "    K values from the overall distances list. \n",
        "    \"\"\"\n",
        "    if len(test) == 0:  # if the test parameter is empty, return an empty list. \n",
        "        print('No test case given. ')\n",
        "        return []\n",
        "    \n",
        "    all_distances = []\n",
        "\n",
        "    test_iter_count = 0  # this counter is used to show progress \n",
        "    \n",
        "    for test_case in test:  # there could be multiple test cases\n",
        "        # extract the test class\n",
        "        test_class = test_case[0]\n",
        "        # extract the test sample\n",
        "        test_case = test_case[1:]\n",
        "\n",
        "        # calculate distances between this test_case and each of training examples. \n",
        "        # distances = distance.cdist(all_train_values, test_case, metric='euclidean') \n",
        "\n",
        "        # NOTE: the get_euc_distance function is replaced with scipy's cdist. see \n",
        "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html\n",
        "\n",
        "        # NOTE: scipy cdist is slower than my implementation, abandoning... \n",
        "        distances = []\n",
        "        for train_sample in train:\n",
        "            train_class = train_sample[0]  # extract the class for later use\n",
        "            train_sample = train_sample[1:]  # now exclude the class from the train_sample var. \n",
        "\n",
        "            # now append (as a small list) the distance between this test case and train sample, \n",
        "            # and this train sample's class. \n",
        "            distances.append([get_euc_distance(test_case, train_sample), train_class])\n",
        "        \n",
        "        # sort the distances array so smallest distance comes first. \n",
        "        # using lambda function meaning for each of the element 'x' in the list, only look at \n",
        "        # the first item in 'x' (shown by x[0], being the distance) when sorting. \n",
        "        distances = sorted(distances, key=lambda x : x[0]) \n",
        "\n",
        "        all_distances.append(distances)\n",
        "\n",
        "        test_iter_count += 1\n",
        "    \n",
        "        curr_percentage = test_iter_count / len(test) \n",
        "        if 0.245 < curr_percentage < 0.255\\\n",
        "            or 0.495 < curr_percentage < 0.505 \\\n",
        "                or 0.745 < curr_percentage < 0.755:\n",
        "            print(f'Total tests: {len(test)}, current: {curr_percentage * 100:.2f}%')\n",
        "    \n",
        "    return all_distances\n",
        "\n",
        "def KNN_optimised(all_distance: list, K: int) -> list:  \n",
        "    \"\"\"\n",
        "    Given a training set, one or more test cases, and an integer K, \n",
        "    predict the class of each test case using K-nearest-neighbours \n",
        "    algorithm. \n",
        "\n",
        "    @param all_distance: the collection of calculated euclidean \n",
        "                distances between all train samples and all test samples (M * N)\n",
        "                \n",
        "                Each row in this collection is the distance calculation for \n",
        "                one test case, ordered by distance ascending. \n",
        "                To use it, take first K as needed. \n",
        "    @param K: integer number indicating how many neighbours to look at. \n",
        "\n",
        "    @return: a list of prediction result, this list has the same size as the \n",
        "                input parameter 'test' \n",
        "    \"\"\"\n",
        "\n",
        "    prediction_results = []  # place-holder list for the classification results\n",
        "\n",
        "\n",
        "    # Now go through all the distances\n",
        "    for row in all_distance: \n",
        "        vote_count = {}  # store the votes as key-value pairs\n",
        "\n",
        "        for res in row[:K]:  # only look at the first K results and vote. \n",
        "            if res[1] in vote_count:\n",
        "                # if this class exists in my count set, update the count value by 1\n",
        "                vote_count[res[1]] += 1\n",
        "            else:\n",
        "                # if not, add this class to the set and give it a value of 1\n",
        "                vote_count[res[1]] = 1\n",
        "\n",
        "        # try to find the max count and its type in the vote_count set\n",
        "        max_count, max_type = 0, None  # start with 0\n",
        "        for key, val in vote_count.items():\n",
        "            if val > max_count:\n",
        "                max_count = val \n",
        "                max_type = key\n",
        "\n",
        "        prediction_results.append(max_type)\n",
        "\n",
        "    return prediction_results"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "source": [
        "results = []  # place holder for K values, and its accuracy\n",
        "\n",
        "# extract all test classes\n",
        "test_classes = [row[0] for row in test_data]\n",
        "\n",
        "# calculate all distances between all train and all test data. \n",
        "all_distances = KNN_all_train_test(train=train_data, test=test_data)\n",
        "\n",
        "print('All_distances calculation complete. ')\n",
        "\n",
        "# Try with K from 1 to 50\n",
        "max_K = 50\n",
        "for k_iter in range(1, max_K + 1):\n",
        "    # use all train available and all tests available\n",
        "    test_res = KNN_optimised(all_distances, K=k_iter)\n",
        "\n",
        "    # compare the test results with the known test classes and calculate accuracy\n",
        "    #  the line below means for each pair (a, b) in the combination of \n",
        "    #  test_classes and test results, check if a == b, if yes, add a '1' to a list\n",
        "    #  finally take the sum of this list of ones. \n",
        "    correct_count = sum([1 for a, b in zip(test_classes, test_res) if a == b])\n",
        "\n",
        "    # append this K, and the accuracy (correct count over all test cases) to the results\n",
        "    results.append([k_iter, correct_count / len(test_classes)])\n",
        "\n",
        "    # show where we are during the process, as well as the accuracy from this iteration\n",
        "    print(f'K: {k_iter} ({k_iter / max_K * 100:.2f}%) -- {results[k_iter - 1][1]}')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tests: 2007, current: 24.51%\n",
            "Total tests: 2007, current: 24.56%\n",
            "Total tests: 2007, current: 24.61%\n",
            "Total tests: 2007, current: 24.66%\n",
            "Total tests: 2007, current: 24.71%\n",
            "Total tests: 2007, current: 24.76%\n",
            "Total tests: 2007, current: 24.81%\n",
            "Total tests: 2007, current: 24.86%\n",
            "Total tests: 2007, current: 24.91%\n",
            "Total tests: 2007, current: 24.96%\n",
            "Total tests: 2007, current: 25.01%\n",
            "Total tests: 2007, current: 25.06%\n",
            "Total tests: 2007, current: 25.11%\n",
            "Total tests: 2007, current: 25.16%\n",
            "Total tests: 2007, current: 25.21%\n",
            "Total tests: 2007, current: 25.26%\n",
            "Total tests: 2007, current: 25.31%\n",
            "Total tests: 2007, current: 25.36%\n",
            "Total tests: 2007, current: 25.41%\n",
            "Total tests: 2007, current: 25.46%\n",
            "Total tests: 2007, current: 49.53%\n",
            "Total tests: 2007, current: 49.58%\n",
            "Total tests: 2007, current: 49.63%\n",
            "Total tests: 2007, current: 49.68%\n",
            "Total tests: 2007, current: 49.73%\n",
            "Total tests: 2007, current: 49.78%\n",
            "Total tests: 2007, current: 49.83%\n",
            "Total tests: 2007, current: 49.88%\n",
            "Total tests: 2007, current: 49.93%\n",
            "Total tests: 2007, current: 49.98%\n",
            "Total tests: 2007, current: 50.02%\n",
            "Total tests: 2007, current: 50.07%\n",
            "Total tests: 2007, current: 50.12%\n",
            "Total tests: 2007, current: 50.17%\n",
            "Total tests: 2007, current: 50.22%\n",
            "Total tests: 2007, current: 50.27%\n",
            "Total tests: 2007, current: 50.32%\n",
            "Total tests: 2007, current: 50.37%\n",
            "Total tests: 2007, current: 50.42%\n",
            "Total tests: 2007, current: 50.47%\n",
            "Total tests: 2007, current: 74.54%\n",
            "Total tests: 2007, current: 74.59%\n",
            "Total tests: 2007, current: 74.64%\n",
            "Total tests: 2007, current: 74.69%\n",
            "Total tests: 2007, current: 74.74%\n",
            "Total tests: 2007, current: 74.79%\n",
            "Total tests: 2007, current: 74.84%\n",
            "Total tests: 2007, current: 74.89%\n",
            "Total tests: 2007, current: 74.94%\n",
            "Total tests: 2007, current: 74.99%\n",
            "Total tests: 2007, current: 75.04%\n",
            "Total tests: 2007, current: 75.09%\n",
            "Total tests: 2007, current: 75.14%\n",
            "Total tests: 2007, current: 75.19%\n",
            "Total tests: 2007, current: 75.24%\n",
            "Total tests: 2007, current: 75.29%\n",
            "Total tests: 2007, current: 75.34%\n",
            "Total tests: 2007, current: 75.39%\n",
            "Total tests: 2007, current: 75.44%\n",
            "Total tests: 2007, current: 75.49%\n",
            "All_distances calculation complete. \n",
            "K: 1 (2.00%) -- 0.9436970602889886\n",
            "K: 2 (4.00%) -- 0.9436970602889886\n",
            "K: 3 (6.00%) -- 0.9451918285999004\n",
            "K: 4 (8.00%) -- 0.9476831091180867\n",
            "K: 5 (10.00%) -- 0.9441953163926258\n",
            "K: 6 (12.00%) -- 0.9446935724962631\n",
            "K: 7 (14.00%) -- 0.9436970602889886\n",
            "K: 8 (16.00%) -- 0.9446935724962631\n",
            "K: 9 (18.00%) -- 0.9382162431489786\n",
            "K: 10 (20.00%) -- 0.9397110114598903\n",
            "K: 11 (22.00%) -- 0.935226706527155\n",
            "K: 12 (24.00%) -- 0.9337319382162431\n",
            "K: 13 (26.00%) -- 0.9302441454907823\n",
            "K: 14 (28.00%) -- 0.931738913801694\n",
            "K: 15 (30.00%) -- 0.9302441454907823\n",
            "K: 16 (32.00%) -- 0.9287493771798705\n",
            "K: 17 (34.00%) -- 0.9272546088689586\n",
            "K: 18 (36.00%) -- 0.9272546088689586\n",
            "K: 19 (38.00%) -- 0.9237668161434978\n",
            "K: 20 (40.00%) -- 0.922272047832586\n",
            "K: 21 (42.00%) -- 0.9217737917289487\n",
            "K: 22 (44.00%) -- 0.9232685600398605\n",
            "K: 23 (46.00%) -- 0.9217737917289487\n",
            "K: 24 (48.00%) -- 0.9207772795216741\n",
            "K: 25 (50.00%) -- 0.9202790234180369\n",
            "K: 26 (52.00%) -- 0.9197807673143996\n",
            "K: 27 (54.00%) -- 0.9197807673143996\n",
            "K: 28 (56.00%) -- 0.9177877428998505\n",
            "K: 29 (58.00%) -- 0.9177877428998505\n",
            "K: 30 (60.00%) -- 0.9157947184853015\n",
            "K: 31 (62.00%) -- 0.9133034379671151\n",
            "K: 32 (64.00%) -- 0.9152964623816642\n",
            "K: 33 (66.00%) -- 0.9138016940707524\n",
            "K: 34 (68.00%) -- 0.9118086696562033\n",
            "K: 35 (70.00%) -- 0.9123069257598405\n",
            "K: 36 (72.00%) -- 0.9128051818634778\n",
            "K: 37 (74.00%) -- 0.9098156452416543\n",
            "K: 38 (76.00%) -- 0.9083208769307424\n",
            "K: 39 (78.00%) -- 0.9083208769307424\n",
            "K: 40 (80.00%) -- 0.905829596412556\n",
            "K: 41 (82.00%) -- 0.9038365719980069\n",
            "K: 42 (84.00%) -- 0.9033383158943697\n",
            "K: 43 (86.00%) -- 0.9038365719980069\n",
            "K: 44 (88.00%) -- 0.9028400597907325\n",
            "K: 45 (90.00%) -- 0.9028400597907325\n",
            "K: 46 (92.00%) -- 0.8993522670652716\n",
            "K: 47 (94.00%) -- 0.898355754857997\n",
            "K: 48 (96.00%) -- 0.898355754857997\n",
            "K: 49 (98.00%) -- 0.8973592426507224\n",
            "K: 50 (100.00%) -- 0.8978574987543597\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions:\n",
        "* What is the accuracy of KNN in the test set for K=1?\n",
        "* What is the accuracy of KNN in the test set for K=2?\n",
        "* What is the accuracy of KNN in the test set for K=3?\n",
        "* ...\n",
        "* What is the accuracy of KNN in the test set for K=10?"
      ],
      "metadata": {
        "id": "6EL9XEpUlUk9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "source": [
        "# answer to the above questions K range from 1 to 10: \n",
        "for i in range(10):\n",
        "    print(results[i])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0.9436970602889886]\n",
            "[2, 0.9436970602889886]\n",
            "[3, 0.9451918285999004]\n",
            "[4, 0.9476831091180867]\n",
            "[5, 0.9441953163926258]\n",
            "[6, 0.9446935724962631]\n",
            "[7, 0.9436970602889886]\n",
            "[8, 0.9446935724962631]\n",
            "[9, 0.9382162431489786]\n",
            "[10, 0.9397110114598903]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In depth questions\n",
        "\n",
        "Extend your algorithm to a multi-class setting (i.e. distinguish between 3 or more digits). How does this change your best value of K?\n",
        "\n",
        "* I started off with multi-class implementation, didn't try the 2-class scenario, thus could not observe the change of best K. \n",
        "\n",
        "If you are familiar with confusion matrices, create one for this test dataset and your “best” value of K.\n",
        "\n",
        "* Best value of K: 4, TP: 0.9477, FN: 0.0523 TN: N/A, FN: N/A\n",
        "\n",
        "Create a plot of accuracy vs. K.\n",
        "\n",
        "* Please see code cell below for the plot. \n",
        "\n",
        "Visualize some of the examples that were classified incorrectly. The examples are 16x16 gray-scale images, so you can plot them on a grid.\n",
        "\n",
        "* Please see code cell below. \n"
      ],
      "metadata": {
        "id": "Q9Tb96s3lUk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot of accuracy vs K"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot([row[0] for row in results], [row[1] for row in results])\n",
        "plt.title('Accuracy vs K')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best accuracy (best K)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "source": [
        "print(max(results, key=lambda x:x[1]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 0.9476831091180867]\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some plots of mistakes"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "source": [
        "# get the classification result for K == 4\n",
        "test_res_k4 = KNN_optimised(all_distances, K=4)\n",
        "\n",
        "# compare with the test sample labels and highlight the index of wrong classifications \n",
        "all_mistakes = []\n",
        "for idx, (actual, test) in enumerate(zip(test_classes, test_res_k4)):\n",
        "    if actual != test:\n",
        "        # if the actual label and the test result doesn't match, append this index to the list\n",
        "        all_mistakes.append(idx)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "source": [
        "print(len(all_mistakes))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "source": [
        "# plot the first 10 wrong classification\n",
        "# create a matplotlib subplot\n",
        "plt.figure()\n",
        "fig, axs = plt.subplots(2, 5)\n",
        "tricky_graphs = []\n",
        "tricky_labels = []\n",
        "# populate a list of tricky graphs and their labels\n",
        "for idx in all_mistakes[:10]:\n",
        "    img = test_data[idx][1:]\n",
        "    tricky_graphs.append(img.reshape(16, 16))\n",
        "    tricky_labels.append(test_data[idx][0])\n",
        "\n",
        "# plot on each of the subplots, and update the plots' x-y coordianates\n",
        "x = 0\n",
        "for idx, img in enumerate(tricky_graphs):\n",
        "    y = idx % 5\n",
        "    if idx == 5:\n",
        "        x += 1\n",
        "    axs[x, y].imshow(img)\n",
        "    axs[x, y].title.set_text(str(int(tricky_labels[idx])))\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis Questions**\n",
        "\n",
        "What values of k did you try?\n",
        "\n",
        "- I tried different K's ranging from 1 to 50. \n",
        "\n",
        "Which value of k produced the highest accuracy? What general trends did you observe as k increased?\n",
        "\n",
        "- Best K is 4, with accuracy of around 94.77 percent. General trend is as K increases, accuracy drops. \n",
        "\n",
        "When using the entire training dataset, what are your observations about the runtime of K-nearest neighbors? List 1-2 ideas for making this algorithm faster.\n",
        "\n",
        "- It is painfully slow when using the entire dataset, as the calculation needed is roughly the number of training set multiplied by the number of test sets, then each of these calculation includes an euclidean calculation of 2 arrays of size 256... so about 7,000 * 2,000 * 256\n",
        "\n",
        "- One idea to improve run time is to use Numpy matrix calculation, essentially turning the nested loop to one giant matrix calculation. Downside of this is it will require quite a lot of memory space, in essence, memory / run time trade-off. \n",
        "\n",
        "- For calculating euclidean distance, we can change it from a simple for-loop to numpy matrix calculation, take all differences between each pair, raise it to the power of 2, sum it, then take sqrt of the sum. (This is implemented in the optimised version of get_euc_distance())\n",
        "\n",
        "- For going through different K values, we can change the KNN() function to first calculate the distance between each training record and testing record, once we have this giant m * n array, going through different K becomes a problem of taking the first K from each of this resulting distance list. (This is implemented in the optimised version of KNN())\n",
        "\n",
        "\n"
      ],
      "metadata": {}
    }
  ]
}